from randomNamemaker import randomName
import matplotlib.pyplot as plt
from keras.optimizers import *
from keras.layers import *
from keras.models import *
import keras.backend as K
from time import time
import numpy as np
import argparse
import os.path

K.set_image_data_format('channels_last')

ap = argparse.ArgumentParser()
ap.add_argument('-b', '--batch_size', type = int, default = 100)
ap.add_argument('-e', '--epochs', type = int, default = 10000, help = 'how many learning epochs')
ap.add_argument('-d', '--dataset', required = True, help = 'path to dataset')
ap.add_argument('-r', '--resizing', type = int, default = 128)
ap.add_argument('-c', '--check_point', type = int, default = 20, help = 'save model every n th epochs')
args = vars(ap.parse_args())

class dcGan:

    def __init__(self, images):
        img_size = images.shape[1]
        channel = images.shape[3] if len(images.shape) >= 4 else 1

        self.images = images
        self.input_shape = (img_size, img_size, channel)

        self.rows, self.cols = img_size, img_size
        self.channel = channel
        self.noise_size = 100

        self.build_dis()
        self.build_gen()

        opti = Adam(lr = 0.00008)
        self.dis.compile(loss = 'binary_crossentropy', optimizer = opti)

        opti = Adam(lr = 0.00004)
        self.dis.trainable = False
        
        # NOTE : adversarial model
        self.AM = Sequential()
        self.AM.add(self.gen)
        self.AM.add(self.dis)
        self.AM.compile(loss = 'binary_crossentropy', optimizer = opti)

    # NOTE : discriminator model
    def build_dis(self):
        self.dis = Sequential()
        depth, dropout,alpha = 128, 0.4, 0.2

        self.dis.add(Conv2D(depth * 1, 5, strides = 2, input_shape = self.input_shape, padding = 'same'))
        self.dis.add(LeakyReLU(alpha = alpha))
        self.dis.add(Dropout(dropout))
        self.dis.add(Conv2D(depth * 2, 5, strides = 2,padding = 'same'))
        self.dis.add(LeakyReLU(alpha = alpha))
        self.dis.add(Dropout(dropout))
        self.dis.add(Conv2D(depth * 4, 5, strides = 2, padding = 'same'))
        self.dis.add(LeakyReLU(alpha = alpha))
        self.dis.add(Dropout(dropout))
        self.dis.add(Conv2D(depth * 8, 5, strides = 2, padding = 'same'))
        self.dis.add(LeakyReLU(alpha = alpha))
        self.dis.add(Dropout(dropout))
        self.dis.add(Flatten())
        self.dis.add(Dense(1))
        self.dis.add(Activation('sigmoid'))
        self.dis.summary()
        return self.dis

    # NOTE : generator model
    def build_gen(self):
        self.gen = Sequential()
        depth, dropout, dim = 256, 0.4, int(int(args['resizing']) / 4)

        self.gen.add(Dense(dim*dim*depth, input_dim = self.noise_size))
        self.gen.add(BatchNormalization(momentum=0.9))
        self.gen.add(Activation('relu'))
        self.gen.add(Reshape((dim, dim, depth)))
        self.gen.add(Dropout(dropout))
        self.gen.add(UpSampling2D())
        self.gen.add(Conv2DTranspose(int(depth/2), 5, padding = 'same'))
        self.gen.add(BatchNormalization(momentum=0.9))
        self.gen.add(Activation('relu'))
        self.gen.add(UpSampling2D())
        self.gen.add(Conv2DTranspose(int(depth/4), 5, padding = 'same'))
        self.gen.add(BatchNormalization(momentum=0.9))
        self.gen.add(Activation('relu'))
        self.gen.add(Conv2DTranspose(int(depth/8), 5, padding = 'same'))
        self.gen.add(BatchNormalization(momentum=0.9))
        self.gen.add(Activation('relu'))
        self.gen.add(Conv2DTranspose(self.channel, 5, padding = 'same'))
        self.gen.add(Activation('sigmoid'))
        self.gen.summary()
        return self.gen

    def train(self, BS = 100):
        # NOTE : pick image data randomly.
        train_imgs = self.images[np.random.randint(0, self.images.shape[0], size = BS), :, :, :]
        
        # NOTE : noise generated by uniform distribution in interval (-1, 1) 
        noise = np.random.uniform(-1.0, 1.0, size = [BS, self.noise_size])
        gen_imgs = self.gen.predict(noise)

        # NOTE : train discriminator
        x = np.concatenate((train_imgs, gen_imgs))
        y = np.ones([2*BS, 1])
        y[BS:, :] = 0
        self.dis.trainable = True
        dis_loss = self.dis.train_on_batch(x, y)

        # NOTE : train generator
        y = np.ones([BS, 1])
        noise = np.random.uniform(-1.0, 1.0, size = [BS, self.noise_size])
        self.dis.trainable = False
        
        # NOTE : adversarial model loss
        adv_loss = self.AM.train_on_batch(noise, y)

        return dis_loss, adv_loss, gen_imgs

    def save(self,genName, disName):

        os.makedirs('genmodels/json', exist_ok=True)
        os.makedirs('dismodels/json', exist_ok=True)

        gen_json = gen.to_json()
        dis_json = dis.to_json()

        with open(f"genmodels/json/{genName}.json", "w") as genfile : 
            json_file.write(gen_json)

        with open(f"dismodels/json/{disName}.json", "w") as disfile : 
            json_file.write(dis_json)

        self.gen.save(f'genmodels/{genName}.h5')
        self.dis.save(f'dismodels/{disName}.h5')

    def load(self):
        if os.path.isfile(f'gen_weights.h5'):
            self.gen.load_weights('gen_weights.h5')
            print('[System] Loading generator weight.')
        
        if os.path.isfile('dis_weights.h5'):
            self.dis.load_weights('dis_weights.h5')
            print('[System] Loading discriminator weight.')

from imutils import paths
from PIL import Image
class DATA:

    def __init__(self):
        size = int(args['resizing'])
        images = sorted(list(paths.list_images(args['dataset'])))
        images = [Image.open(images[img]) for img in range(len(images))]
        images = [images[img].resize((size, size)) for img in range(len(images))]
        images = [[np.array(images[img]).astype('float32')] for img in range(len(images))]
        self.x_train = np.vstack(images) / 255.0
        # print(self.x_train.shape)

dataset = DATA()
data = dataset.x_train

gan = dcGan(data)
gan.load()

EPOCHS = int(args['epochs'])
SS, BS = 10, int(args['batch_size'])
train_per_epoch = data.shape[0] // BS

for epoch in range(0, EPOCHS):
    total_dis_loss = 0.0
    total_adv_loss = 0.0
    ckpt = int(args['check_point'])
    imgs = None

    for batch in range(0, train_per_epoch):
        startTime = time()
        dis_loss, adv_loss, t_imgs = gan.train(BS)
        Time = time() - startTime

        total_dis_loss += dis_loss
        total_adv_loss += adv_loss
        if imgs is None:
            imgs = t_imgs

        total_dis_loss /= train_per_epoch
        total_adv_loss /= train_per_epoch

        print(f'[Calculating...] Epoch : {epoch}, Dis Loss : {total_dis_loss:.4f}, Adv Loss : {total_adv_loss:.4f} ({Time:.3f} / batch)')

    if (epoch % ckpt == 0) or (epoch == EPOCHS - 1):

        fig, ax = plt.subplots(1, SS, figsize = (SS, 1))
        for idx in range(0, SS):
            ax[idx].set_axis_off()
            ax[idx].imshow(imgs[idx].reshape((gan.rows, gan.cols, gan.channel)), interpolation = 'nearest')

        if not os.path.isdir('genImgs'):
            os.mkdir('genImgs')

        fig.savefig(f'genImgs/gan_{epoch}epochs.png', format = 'png')
        disName = f'disModel_{epoch}epochs'
        genName = f'genModel_{epoch}epochs'
        gan.save(genName, disName)
        print('[System] Model saved!')

    if epoch == EPOCHS:
        for img in imgs:
            cv2.imwrite(f'genImgs_{randomName(5)}.jpg',img)
        
        print('[System] All Generated Images saved!')



